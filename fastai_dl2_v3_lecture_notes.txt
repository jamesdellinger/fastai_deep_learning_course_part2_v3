PART II: Deep Learning from the Foundations

Lesson 8
Building Optimized Matmul, Forward and Backpropagation from Scratch
Monday, March 18, 2019
----------------------

-this will be a different kind of part II than previous years' versions fastai DL part II
-we will implement much of fastai from Foundations (basic matrix calculus, training loops w/ callbacks, annealing, regularization, dataset loader and blocks)
-lots of layers and architectures
-jupyter dev testing and docs
-read and implement papers
-learn object detection, seq2seq etc.
-study performance such as parallel GPU training etc. JIT, Cudnn
-last two lessons: will implement some of fastai in Swift

-reason for the change from older way of doing Part II: so many papers are coming out now, so it's impossible to select the canonical papers to read. It's better to learn the foundations so you can read and then implement whatever paper you want.

-If part I v3 was top-down, we could say Part II v3 is bottom-up. We don't mean bottom-up from math (which is what people usually mean when they say "bottom-up"). We mean bottom-up from code, such as implementing matrix multiplication in python.

Why are we doing Swift? Cause Chris Lattner is focusing solely on deep learning, and he understands how to build programming languages. If you look at TensorFlow, you can tell it was built by DL people, not by people who deeply understood compilers. Chris Lattner is the first time someone from the programming langauge world is getting involved in DL. That's why he's gonna join us. 

Julia is another good language like Swift (and it's further along than Swift).

New compiler project MLIR that Chris is involved with that could be important in the long-term:

"Multi-Level Intermediate Representation - a new intermediate representation designed to provide a unified, flexible and extensible intermediate representation that is language-agnostic and can be used as a base compiler infrastructure." (https://llvm.org/devmtg/2019-04/talks.html)

Foundations = recreate fast.ai, matrix mult, torch.nn, torch.optim, Dataset, DataLoader, but make it even better.

Then final 2 weeks, we'll see if Swift can do it as good or better.

The game we'll play: we can use fastai or Pytorch modules/methods once we're implemented from scratch in our course here.

Will get us ready to contribute to libraries. 

Another good thing about Swift for Tensorflow: it's early days but there is much long-term promise. We can get in early and if we want, become key contributors in the development and evolution.

In Part II your opportunities are at the cutting edge and you have much more opportunities to blog. Try to do experiements and much of what you find will not have been written before. Write stuff for the you of six months ago and blog and don't try to make it perfect before you hit publish!

Steps to train a good model:
1. Overfit (you've seen your validation error get worse)
2. Reduce overfitting
3. There is no step 3.

Five steps to avoid overfitting (in decreasing order of importance):
1. More data
2. data aug
3. generalizable arch
4. regualrization
5. reduce arch complexity (should be the last thing you try)

For papers: often easy to find a blog post or tutorial that does a better job of explaining a paper than the paper does (I could write some of these posts)

Goal for these next few lessons, implement from scratch all the components necessary to make a ResNet CNN

First step toward this goal: implement Matmul

matrixmultiplication.xyz shows what happens when multiply matrices

broadcasting allows us to get rid of nearly all our loops. Idea of broadcasting goes back to APL in 1962. 

idea is to replace for-loops with broadcasting loops.

You can create tensors that behave like higher-ranked things than what they're actually stored as in memory. (.storage() and .stride())

unsqueeze() is the same as indexing by the special value None (means: please squeeze a new axis in here please)

MLIR is based on a language called Halide which showed a way to make flexible and highly optimized language for lin alg

PyTorch gets their matmul so fast by pushing it out to BLAS (library made by Nvidia or Intel that's optimized for running lin alg on their hardware (like a GPU or CPU))

Reading papers from competition winners is a great idea! Competition winner papers tend to have like 20 good ideas (such as Kaiming He's paper that introduced ResNets, Kaiming He initialization, etc.

Kaiming init: accounts for the fact that Glorot initialization doesn't account for having a ReLU activation function.  Cause Relu takes away half the dev. by removing all values less than 0.

fan-in: keep variance at 1 thru the forward pass
fan-out: keep variance at 1 for the backward pass

in PyTorch, capital F always refers to torch.nn.functional

squeeze() gets rid of unit axes (be sure to specify index of dimension you wish to squeeze so that in case you have a batch of size 1 it doesn't also get squeezed out)

The Matrix Calculus you Need for Deep Learning -- paper by Jeremy and Terence

most important part of matrix calculus for DL: the chain rule

inside PyTorch classes, __call__() is used like forward() used to be used

HW Lesson 8:
-Convince myself why matmul with broadcasting works (look at the cell in matmul nb where it's defined) (use excel if necessary to show each step)
-Understand why kaiming he initiation works better (nb 02)
-read section 2.2 of the resnet paper
-read Jeremy and Terence's matrix calc for DL paper
-Reimplement Lesson 8 NBs (1, 2)


Lesson 9
Deep Learning from the Foundations: How to train your model
Monday, March 25, 2019
----------------------

A lot of this course will be Jeremy showing how he does research and how he does software development; and he hopes that'll be useful for us

See conv example spreadsheet from part I if you want to see why filter size for first conv layer is 5, and why weights tensor is (32,1,5,5) (in 02.whysqrt5.nb.ipynb)

Kaiming init designed to be used after ReLU or Leaky ReLU layers

PyTorch default init doesn't give mean=0/std=1

fan_in = number input filters x receptive field size
fan_in =number of output filters x receptive field size

PyTorch used Kaiming uniform init by default (not Kaiming normal). They chose sqrt(5) for gain because they needed a gain that would be close to the std dev of a uniform dist in [-1,1]. 

You want mean/std dev of all layers' weights to be similar -> when you multiply matrices over and over, it's not too long until the results explode so big your computer can't even recognize the result as a number.

Carefully choosing starting weights prevents this from happening -> You want your gradients to not be 0 or infinity

Anyways, PyTorch wasn't properly init-ing their layers according to current best practices. Jeremy found this out in the research done in the 02.whysqrt5.nb.ipynb notebook. Turned out it stemmed from a "good bug" (or so Soumith et al thought at the time) that had been introduced in the original Lua implementation of Torch some 15 years ago.

Jeremy demonstrated that PyTorch's default init wasn't ideal and the PyTorch team now has a ticket to update this.

Moral of the story is: don't just assume that libraries do everything the best possible way -- there may be a bug!

As Jeremy says: "when it comes to deep learning none of us knows what they're doing"

Jeremy finds often: "In deep learning something's done a certain way, and it just gets done that way forever and no one challenges it."

Last week: we finished up to forward/backward pass of a fully connected network. 
This week: we're ready to write code to train our model.

We can write cross-entropy loss as -log(p-sub-i) where i is the index of the desired target -> this technique is called integer array indexing -- pass two lists (first contains row indices, second contains column indices)

make sure to do gradient update with torch.no_grad() because the updates are what you have as a result of the gradient calculation

Python's __setattr__ method lets you make something happen when a particular attribute is set.

Jeremy doesn't like using random seeds while developing models. Although reproducibility is important once models are all made, while you're still building them, you should allow for some randomness and variation -- so you can develop an intuition for what parts are stable and what parts aren't.

Python always calls __iter__ when you loop through something, so we can just create a __iter__ method in our DataLoader() class which yields each batch.

You want to get your code to a point where you can read it very intuitively ... makes it much easier to maintain. Helps for research too -> allows you to try more things cause your code is more extensible.

Most of the time you don't need to worry about specifying your own Sampling and Collation function when using PyTorch's dataloader.

Some kinds of layers need to have different behaviors depending on if they're training or not. Eg. dropout only does randomized dropout during training and not during evaluation.

If batch size varies you need to be sure to take this into account when printing out metrics during training.

Why we have to zero out our gradients in PyTorch: our gradients will keep getting bigger and bigger with each pass/step.

But we can't zero the gradients inside a step, cause sometimes we won't want to zero out our gradients (like if we have a super small batch size), so we call a designated zero_grad() method after we call step(). And if we are in a situation where we don't want a zero gradient and we want to accumulate gradients, we can do that.

Fastai's callbacks let you look at and fully customize every single phase of training.

Allows us to make lr schedulers, early stopping, parallel trainer, gradient clipping, etc.

TestCallback() like class is useful when you want to run just a few rounds, not an entire epoch, just so you can test out that things are working.

Runner() is a new class that contains: one_batch, all_batches, and fit. Will probably appear in a future version of fastai. 

Duplicate code is bad cause there are a bunch of places you need to edit if you wanna change something; also typically harder to read and understand.

Rule of thumb with code: if you're gonna do something multiple times, make it as small as possible. This is why the __call__ function in the Runner() class is the smallest method in the class.

What if you want to add callback after you activate the forward pass of the second layer of your model? THis is why PyTorch has Hooks -- they are callbacks that you can add to specific PyTorch modules.

Happens in fastai a lot: when one object contains another object, we very often *delegate* an attribute to the object that is contained.

camel2snake: converts TextLikeThis to textlikethis

understanding how fit() function works is the most important part of NB4

"In last 12 months one of successful areas of research has been people pointing out that you can and should schedule everything."

Eg. dropout scheduling, augmentation scheduling. 

This might be an interesting area for me to explore -- what other areas can be scheduled?

fastai calls it layer_groups, PyTorch calls is param_groups

Jupyter is good at Python dynamic code generation necessary to understand function decorators

create an annealer decorator by defining an _inner() method inside it, which returns a partial()

putting a monkey patch on torch.Tensor (called ndim) lets you plot tensors with matplotlib.

cosine 1-cycle schedules: best way to do one-cycle currently cause we need to stay at a higher learning rate for a good while, and then fine-tune/warm-down for a good while as well.

HW Lesson 9:
-Reimplement lesson 9 NBs (2a, 2b, 3, 4, 5)
-Read papers (SeLU, All you need is a good init, Exact solutions to the nonlinear dynamics of learning in deep linear neural networks)


Lesson 10
Deep Learning from the Foundations: Wrapping up our CNN ... nearly.
Wednesday, April 3, 2019
----------------------

Don't write about running batch norm until the latest part two MOOC is released (June, 2019).

Don't feel bad for not keeping up w/ papers and lesson notebooks! Jeremy says he's trying to give us enough to keep us busy until PART II OF NEXT YEAR!

This course also has a lot of software engineering-y stuff to help data scientists become better software engineers.

The goal within the next week or two is to have our homegrown CNN get at or near SOTA on ImageNet

Callback is a concept. GUI programming uses them, when an event happens. "Please call back to me, when something happens."

Python and most languages have a way to take a function that accepts two parameters and turn it into a function that only needs one parameter. In Python the way to do this is to call a function with partial()

__call__ is a method that will be run if u create class object and call it with parentheses

for machine learning, mean absolute deviation is what you really wanna be using (instead of standard dev) cause it's much less sensitive to outliers (cause std dev squares the distance from the mean, which really exaggerates outliers). And over-exaggerating outliers can mess up your machine learning models.

Softmax isn't ideal when you are in situations when your model may need to predict that zero items belong to any class, or when your model may need to predict multiple items, each belonging to different classes -- for finding objects in images. 

Honestly for most image recognition we don't want softmax, cause an image likely will have none or multiple classes present.

We all use softmax cause we grew up using ImageNet which only has one class for each of its images.

An alternative is to use sigmoid to make binomial predictions for each class -- either an object belonging to a particular class is present in that image, or it isn't.

Exceptions aren't just for errors -- they can be useful for control flow

For example, inside run.fit() we raise an exception after 10 rounds to cancel training.

First layer of CNN has filter size of 5 cause we have input channel size of 1 and output channel size of 8 and if we had chosen a kernel size of 3 like we do for all our other layers: 
    -Well, we know we have 8 filters in our first layer. 
    -our input image has a single channel. if we used a 3x3 kernel, there are 9 input activations it'd looking at at any given time.
    -it splits those into 8 dot products (cause there are 8 filters), which gives a vector of length 8. 
    -This would be pointless cause we would have begun with _9 numbers_ and only ended with _8 numbers_,  So it'd be a pretty useless computation that's shuffling the numbers into a different order.
    -This is why using a filter size of 5 actually gets our first layer to do something useful

Similarly, most ImageNet models make the first layer filter size 7x7, and then 3x3 at all subsequent layers.

When u look at an architecture, always think about how many numbers are going into a dot product, and is it serving a purpose (or only doing busy-work, as would have been the case had we used a filter size of 3x3 instead of 5x with our MNIST images.

PyTorch calls them "Hooks," not "callbacks," but they're the same thing

After you're done with a hook module in PyTorch, be sure to call hook.remove(). If you don't do this, and keep calling subsequent hooks, you'll eventually run out of memory.

The methods in our ListContainer() class work better than numpy

It can be helpful to see what pctg of activations in the final layer are ZERO -- being totally wasted. We'll never get optimal results by wasting so many of our activations.

maxv is maximum value for our GeneralRelu() class, in case we felt it'd be better to have a maximum value. 

Lots of people think that drawing from a uniform random dist is better than normal. But at least in my experiments I haven't found this. If anything, I've found that normal works a bit better, all else being equal.

If a layer has good accuracy, what can you find out about the telemetry of the layers? What characteristics do layers have that are observed when model accuracy is at its best?

Batchnorm has been around since 2005.

batchnorm parameters gamma and beta can learn.

At inference time we can't normalize (batchnorm) the same way we do during training. We need to keep an exponentially weighted average of the means and variances of the last two batches. We use this running average at inference time.

self.register_buffer('vars') is better than self.vars. Cause everything we register as a buffer is automatically moved to the GPU if we decide to run our model on the GPU (not so if we did self.vars).

Also if we save a model, we have to save the running means and variances. If we register them as buffers, then we can save them when we save the whole model.

Taking the exponentially weighted moving average is way more computationally efficient than taking a traditional moving average. Important in large models that have potentially hundreds of millions of activations (no way could ever hope to save all of them.)

linear interpolation in PyTorch is called .lerp()

Remember that PyTorch's lerp uses the exact opposite of indicating momentum. If we have a momentum of 0.9, we have to tell lerp that our momentum is 0.1

Batchnorm allows us in our example to get to a learning rate of 2.

Batchnorm is problematic with small batch sizes cause u can get variance of 0 and a norm of infinity. Cause it'll be more likely you end up in a situation where the value is same as or very very close to the mean.

Start out by trying normal batch norm unless you have really large batch sizes
If you have large batch sizes, try running batch norm
If you are using RNNs, try group norm

Foggy days: layer norm would cause means (really?) and variances (may only be just the variances) of hazy days and sunny days to normalize to the same. Anywhere where the overall level or difference of activation is part of what you care about, layer norm is designed to throw it away. 

Moral of the story: layer norm is a partial, hacky workaround to some problems with batchnorm. But as the hazy/sunny day thought exercise shows, it has potential problems of its own.

Instance norm wasn't designed for classification. It was designed for style transfer.

You can't go in and randomly try another normalization scheme without knowing what it's for and what some pitfalls may be.

However, none of these attempts are as good as batchnorm.

But there may be a way to fix the small batch size problem with batch norm. Use epsilon, to prevent dividing by zero, but why not make epsilon trainable!

But there's a better idea: Running BatchNorm. The first _true_ solution to the small batch size problem. It's a ridiculously simple thing: in the forward function for running batch norm, don't divide by the batch standard deviation and don't subtract the batch mean. Use the moving average statistics at training time as well, not just at inference time.

Why does this help? Imagine you're using a batch size of two, you may get two values that are close together and have a variance of zero. Then if you use normal batch norm you'll be dividing by zero. But with running batch norm, you only take 0.1 of that and 0.9 of whatever you had before so won't throw things off. As long as you don't get really unlucky and have the first batch be dreadful. As long as you use the running average (exponentially weighted average), you won't have the problem of dividing by a zero variance.

Many details need to be taken care of for this to work:
    1. You can't take the running average of variance --  to do so simply wouldn't make sense.
       Instead, we calculate variance by E[X^2] - (E[X])^2
           - keep track of the sums and squares as registered buffers
           - use lerp to take the exponentially weighted moving average of the sums and squares
    2. Batch size could vary, so register a buffer for count, and take an exponentially weighted 
       moving average of the counts of the batch sizes.
    3. Must do debiasing on sums, squares, and count: want to make sure at every point that 
       no observation is weighted too highly. In the normal way of doing moving averages, the 
       first point gets far too much weight. We can fix this by initializing sums and squares 
       to zeros, and then using lerp to calculate the exponentially weighted moving average. 
       We have to divide by 1 - momentum^n, where n is the index of the observation.
    4. Clamp the variance to be no smaller than 0.01 if 20 items or less have been seen. 
       This prevents getting a really low value for variance in the unlucky event that 
       the first two (or few) items in the batch are identical or very close in value 
       to each other.

It's really good to come up with toy problems in research and find solutions to them. 

HW Lesson 10:
- Attempt toy problem of getting the best accuracy you can in one epoch, using whichever 
  normalization scheme you prefer, in the 07_batchnorm notebook
- Reimplement all lesson NBs and read papers.


Lesson 11
Deep Learning from the Foundations: Data Loading, Optimizers, and Augmentation
Wednesday, April 10, 2019
-------------------------

Added simplified running batchnorm to 07_batchnorm.ipynb notebook. Turns out the de-biases cancel each other out so we can remove them from the running batchnorm function.

Also refactored the way of computing self.factor and self.offset. 

It can be finicky getting your network to keep a unit variance through all its layers. LSUV (layerwise sequential unit variance) attempts to solve this. 

LSUV module just uses while-loops to keep the std and mean within acceptable tolerance of unit mean/var.

Beauty is that thanks to LSUV you can initialize any kind of network you want without having to worry about compensating for what specific layer types you have (like ReLU) and ensure you still have a unit variance and mean.

You run LSUV once at the start of training.

Jeremy feels like LSUV embodies the fastai approach to initializing -- no math no thinking just do what works and is simple and straightforward.


Cifar-10 vs. Imagenette: we need Imagenette cause using Cifar-10 images are too small (32x32) to be a good comparison for model perf. on ImageNet-sized images. Instead, using Imagenette which just has 10 classes chosen from ImageNet, and keeps ImageNet image size.

Jeremy finds that what works well for ImageNet also works well for Imagenette. 

A big part of getting good at using deep learning in your domain is learning how to create small, workable datasets to test/tinker with models on, like Imagenette.


Datablock API: first step is to find the library that can read in whatever kind of training data you are using. We use PIL here to read images.

Tench is the first category in ImageNet. Anyone who is a deep learning practitioner knows how to recognize tenches.

os.walk() / scandir functions in Python is super fast for recursing through directories -- it's a very thin wrapper over the C API. Scandir should be much faster than glob.

Jeremy sees this course as him sharing with us his research journal over the past six months

self.otoi = object to int

Make sure you use the same mapping (ie. vocab) in both your training set and validation set. Failure to do so is the main cause of people finding their model's performance on their val. set is no better than random.

c_in and c_out (channels in/out) allow you to specify your model's input size and output size.

We want to make sure our first convolution has something useful to do. Instead of using filtersize of 5. We could use filtersize of 3, and as a rule of thumb, and according to bag of tricks paper, we pick output channels to be nearest previous power of 2 to the product of input channels * kernel area (3x3). so if we have 3 input channels, and a 3x3 kernel, we need to find nearest prev power of 2 to 3x3x3=27, which is 2^4 = 16. So we want our first conv layer to have 16 output channels with a 3x3 kernel. We know that 3x3 kernels are better than beginning with a 5x5 or 7x7 kernel in the first conv layer, which is unfortunately what many practitioners do. 


There is one optimizer, just different ways to tweak it -- Jeremy says this is how we ought to think about optimizers.

Parameter groups are the same as layer groups. Jeremy says in fastai it makes sense to call them parameter groups (as is already done in PyTorch) instead of layer groups (which is what fastai had been doing)

If we make params a list of list, then that lets us do parameter groups.

p.grad.data.add(wd, p.data) in PyTorch adds p.data to p.grad, but first it multiplies wd times p.data, then adds that product to p.grad

Momentum needs to know every single parameter, and how much did it move last time. In other words, momentum needs to be able to keep a state. This is why momentum is gonna need a slightly different optimizer from SGD. 

We need not only exponentially weighted moving average, but also debiasing when we use momentum. For debiasing, if data not randomly distributed, if the data follows a function, then momentum will rely too heavily on earlier values and be too slow to catch the overall trend. Debiasing counters against this. Debiasing: 1-mom^num_steps / (1-mom)

If there's Batchnorm, how could L2 regularization actually work??

Twan van Laarhoven mentioned this in middle of 2017 (1706.05350) and nobody noticed. Turns out WD does do something, and Twan was wrong, but we still don't know what it does. 

To sum it up, no one in the community really knows what L2 regularization does. The takeaway for us students is: we can contribute to deep learning, cause guess what: no one has any idea what they're doing. 

If you can combine thinking about theory, with the experiments of being a practitioner, you can find some interesting results. Most folks either are practitioners and forgot about how to think about the theory, or are theoreticians and don't actually ever train models!

When you refactor your math it's much easier to see what's going on. Mathematicians hate refactoring their math -- don't be like them!

Most people use epsilon=1e-7 for adam-w. There's no reason for this. 0.1 may be better.

LAMB is basically ADAM, but we're averaging the steps out over a whole layer. It's really unlikely that every individual parameter in a tensor should be divided by its gradients. There's too much variation.


Image augmentation should be:
    - good quality
    - fast

Best way to tell how well augmentation is working is to simply look at the images produced. Are things that are supposed to be clear and well-defined still clear and well-defined? Such as the plaid shirt of man holding the tench fish should still appear as a plaid shirt after having been resized. 

If you're flipping an image, flipping bytes is way faster than floats, cause floats are slow. Moral: do as many augs in bytes as u can. Some augs, however, such as saturation, can't be done effectively in bytes.

Every ImageNet winner has used random resize+crop as their main augmentation. It's thus arguably the most important aug. But it's slow so you have to be careful about how you do it. 

Label Smoothing is tailored to work well with mixup. But Jeremy thinks what we really should do when we have mixup is use a mixture of two softmax losses. There's a mixture of softmax paper that goes into more detail. 

In NLP, really useful thing to do is grab different sized chunks of the text to serve as a data aug. Similar thing in audio to grab different lengths of audio as aug inputs. The concept extends across several domains.

Perspective transforms/image warping make much more sense than squishing, lengthening images. Cause squishing/lengthening isn't something you'd every physically do to objects. But perspective changes would conceivably happen. Bigger principle: try to make your augmentations perform what would be physically realistic in the real world. Those kinds of augmentation, such as perspective (warping) transforms.

All the research we have tells us that we can handle labels that are missing or wrong (are noisy). The only thing that matters is that the labels aren't biased.

Amount of time it takes to convert image to a float tensor is much longer than the amount of time it takes to warp an image (it being kept in bytes). Moral of the story: time everything.

Transforms should happen after you create a batch. That way you can do batch transforms on the GPU. Rotation is just a matrix multiplication, afterall. PyTorch has an optimized grid interpolation function: F.grid_sample(). (most libraries do transforms one image at a time on the CPU).

Reflection padding gets rid of the black spaces around images that were rotated. This helps performance.

HW Lesson 11:
- Review LAMB papers, and papers that debunk the idea that L2 reg (WD) provides regularization.


Lesson 12
Deep Learning from the Foundations: Transfer Learning and NLP
Wednesday, April 17, 2019
-------------------------

We are wrapping up all the pieces for computer vision. And we will also both begin and warp-up NLP.
Debugging machine learning code is "awful" so endeavor to write your code as simple as possible.

Mixup is just taking the linear combination of two images and their corresponding labels. If we have 70% tench and 30% french horn, we want to predict 70% tench and 30% french horn as the label. The correct answer would be having a rank-1 tensor with a .7 in one spot and a .3 in the other spot and 0 everywhere else.

Note that at this point it's ideal to have label smoothing -- an ideal way to handle noisy labels.
People have randomly permuted half of the labels and used label smoothing and STILL got really good results.

Nice thing about mixup:
    - doesn't require any domain specific thinking
    - not lossy
    - infinite in terms of numbers of different images it can create

Note: Loss functions in PyTorch have reductions. You can return all the elements together or just reduce them by summing or taking their average.

Having a context manager makes it possible to implement mixup.

Half precision floating point (fp16) in theory is 10x faster in reality it's not *quite* that fast but still an improvement.

XResNet: some debate about whether "X" means mutant or extended lol

XResNet is basically the bag-of-tricks version of ResNet:
    - ResNet-C from the paper. Do three 3x3 convs in a row in each block.
    - bn weights sometimes initialized to 0 and sometimes to 1. 0 if it's the 3rd conv layer in a block. This allows to train deeper nets.
    - expansion param is 1 if resnet 18/34, and 4 if deeper resnet

Doing a stride 2 on a 1x1 conv is a terrible idea. You're literally throwing away 3 points of data.

The same learnings that help Google do giant batch sizes can help us use same batch size but HIGHER learning rates. So that's why we care about how to optimize training with large batches.

Preact resnet doesn't work as well for smaller models (the final relu goes after the addition at the end of the block).

Nvidia graphics cards like things being in multiples of 8.

For research you want to refactor your architectures to be very concise so you can verify:
    - it's right
    - think about what would happen if you changed some things

Imagewoof is harder than imagenette and it's a better test of your model.

Use good common sense. No need to run to architecture/hyperparameter search.

Did transfer learning from Imagewoof-trained model to pets dataset from lesson 1.

Use ordered dict to store keys/weights for each layer. 

Cut the XResNet just before the avgpooling happens! Then attach the head to that. But how to find number of connections? Just use the cut xresnet to make a set of predictions and see how many outputs there are.

RUle of THumb: Anytime something weird happens in your neural net: it's likely cause of batchnorm.

Freeze all body parameters that aren't in batchnorm layers. DOn't freeze weights in batchnorm layers when you're doing partial layer training.

Make sure you can check your code you implemement. Such as having a debugger callback to verify you indeed *are* using discriminative learning rates. Assume you will have mistakes in what you wrote.

Cross validation is a useful technique for creating a reasonably sized validation set if you don't have enough data to create a reasonably-sized validation set. But it was for the era when you maybe had 50-60 samples. Now when we have thousands of samples, we simply don't need it.

how to debug deep learning: make your code so simple that it can't have any mistakes. Otherwise you might have what happened to Jeremy... debugging ImageNet training loop to the tune of $6,000 worth of AWS credits being spent.

AS a DL researcher, have a good journal notebook. A record of everything you ran and the results. So you can always reproduce previous results if needed!!! When you look at the great scientists in history, they all had careful scientific journal practices. In Jeremy's case he logs everything in a Windows Notepad file. Contains what he's doing and what the results are. He uses ctrl-f and searches for things. Scientists look at something that they think shouldn't be there and go "hmm, why is that there?" and then they study more and they find something interesitng. Like studying the bubble in the beaker that shouldn't have been there and discovering Nobel Gases. All Jeremy's fiddling around -- 90% of it doesn't go anywhere. But the 10% does. 

Jeremy: "DL is tough and u shouldn't do it but on the other hand it gives better results than anything else and it's taking over the world." lol.

When testing, you don't want reproducible tests. You want randomness in your tests. Ones that aren't guaranteed to always pass.

Now we turn to NLP:

Lots of interesting things have been happening. GPT-2 in openai, BERT, ELMO, transformers, etc. Seems like LSTM stuff falling out of favor. But it's not true that RNNs are in the past. ULMFiT is doing really well too. Transformers have their own problems. THey don't have state. 

But RNNs even tho they have state they're fiddly. But there's been much more studying in how to regularize RNNs than Transformers. Stephen Merity went and found all the diff. ways that regularization works.

People in industry aren't finding BERT/Transformers are as good yet for real-world tasks.

Lots of SOTA results in applying RNNs to genomics/chemical bonding/drug discovery. These things all have sequences.

"Language Model" is just a very general term for "predict the next item in the sequence"

Language models and classification models require two diff kinds of preprocessing.

spacy is awesome for tokenizing.

Preprocessing: make things as easy as possible for your LSTM to keep track of the state. Instead of repeating 28 exclamation marks in a row, just have a heuristic for displaying the length of repetition more concisely. Something like "rep 28 !"

Why we have BOS/EOS (beg of string/end of string): LSTMs need to know when to reset state. 

How do we batch up language model data:
    - each batch has the same document over consecutive timesteps.
    - from batch to batch, the state that the lstm is building up needs to be consistent.

Don't ever remove info from your neural net that might be useful. Stopwords are very useful don't get rid of them. Don't stem cause it'll get rid of the tense of the words. "Stemming is in the past. Rule of thumb: leave your text data as raw as you can."

Great experiment: exploring tradeoffs between longer bptt and smaller bs VS. shorter bptt and larger bs

Pad each document by adding an arbitrary padding token. Ssort your validation batches by length of the documents they contain. First batch has the longest ones. Last batch has shortest. 

For training tho, you want randomness, so do a sort-ish sampler that approximately sorts longest to shortest but not quite.

between every layer in RNN we use the SAME weight matrix.

LSTM at its core: selectively update some things, and selectively forget things. You can do this other ways (like GRU). Key thing is make sure your cell has a way to selectively forget things.

JIT takes Python and converts it into CUDA C++ that does the same thing. Makes things stay and run way faster on GPU. Not as fast as CUDA BLAS but faster than on CPU.

BUT: Jeremy says it's been a nightmare trying to get JIT working. He doesn't think it's the future. He's found places where it gives the wrong gradients (goes down the wrong autograd path) and gives NO warnings.

Weight dropout in AWD-LSTM actually drops out weights.

Gradient clipping not used as nearly as much as it ought to be -- allows you to use higher learning rates.

Try not to have things that massively change from timestep to timestep: the goal of regularizing an RNN.

Why Swift for TensorFlow? Python's days are numbered. JIT is an example of failed software dev processes he's seen over past 25 years. Believes Julia or Swift in the coming years will take over. WHY? we can't write RNN cells in Python. WE can't write CUDA kernels in Python.

EVerytime Jeremy learns a new language he becomes a better developer.

Swift community needs our skills -- they don't understand deep learning. 

S4TF goal is be subtractive of complexity.

Chris believes that due to Python the application world of deep learning and research world are too bifurcated. Once we get the compiler (MLIR) figured out from the bottom up we can much more easily deploy deep learning include it in real applications.

One of Swift's goals is to be a full-stack langauge from scripting all the way down to the low-level performance stuff C++ is good at. So u shouldn't long-term need to use C++. 

HW Lesson 12:
-When you read the bag 'o tricks paper, think about the ResNet tweaks they outline and see if you can figure out WHY they made them.
-Read mixup paper it's easy to read.
-Interesting assignment is add distillation to the notebook. We have all the tools necessary with knowing how to do callbacks.


Lesson 13
Deep Learning from the Foundations: fastai + Swift for TensorFlow, part 1
Tuesday, April 23, 2019
-----------------------

Confirmed: with S4TF you currently need a system-wide CUDA toolkit/CUDNN/Nvidia drivers

Use Harebrained Install guide cause it guides you how to install CUDA/CUDNN without it automatically updating your Nivida drivers without your consent

fastai history: used to use Keras+TensorFlow but had to stop cause they couldn't build what they wanted to build with them -> then began PyTorch, that worked well for a while, but realized couldn't teach it to new practitioners, so created user-friendlier fastai library -> but now want to create new things that are hard to do on PyTorch (have to use JIT which is flaky or write in CUDA which no one wants to do), so now it's time to build the next thing -> very likely chance that this could be Swift for TensorFlow

Why not Julia? It's not just about the language it's about who's using it... Julia doesn't have a big customer that needs it to work well...but Swift for TensorFlow has Google and Google needs it to work.

Jeremy's spent time with Julia creator folks and still feels that Swift is aiming for next level things above what Julia is aiming for.

Chris Lattner: "Swift is ambitious/conceived as a full-stack programming language from the start/other languages weren't made with this intent"

Swift purposefully designed to look familiar. Lots of languages start out being all about "trying to prove a point about something" but Swift intended to use the best practices of everything, but while still being carefully curated/designed.

S4TF goal: make an open, hackable platform where u can change anything u want in it, so you can do research.

Chris' S4TF team develops new features like autodifferentiation on a dev-fork branch from Swift and then once the features are built out, they try to have them merged back into the main Swift language. This has already happened several times.

way to think about Swift structs: are more efficient than classes in Python (in terms of memory allocation), not like C structs at all, think of it conceptually as being closer to a Python class

defining custom prefix operators like square root is an example of Swift being very hackable

Swift Python interop currently super important cause Swift has basically zero data sci libraries. But over time hopefully this will change as more stuff gets written in Swift. Jeremy hopes that S4TF community will write Swifty counterparts of libs like Pandas, matplotlib, etc. Swift was designed for building APIs.

with Swift Jupyter NBs, not as reliable as Python ones so if something you expect to be working is working well, first strategy always should be to just restart the notebook

LLVM powers Julia, Rust, Swift, Clang

LLVM can actually pull expressions out of a for-loop automatically and make it not run needlessly sequentially over and over.

Swift doesn't have magic, built-in things. the Float type for e.g. is a public struct

var myArray: [Int] is just syntactic sugar for Array<Int>

Operations on arrays (e.g. map/filter/reduce) can be done with closures cause they are implemented vectorized already. You don't need to do weird syntax like you do with Python (e.g. list comprehension)

S4TF has a tensor type e.g. Tensor<Float> or Tensor<Int>

Swift implementation of nested matmul with 3 for-loops takes .13ms while the Python version took 835ms!!!

So with Python we had to do einsum or use the BLAS

Moral of the story: you can ship in the language you ideate and experiment with. You don't have to re-implement in C like you would do with Python machine learning code.

Swift on Server is Swift analogue to flask/django. Vapor is another one.

Swift generics reduce duplicate code

If generics frustrate you, start by writing duplicate implementations of the method for different datatypes. Then eventually try and refactor into one method that uses generics.

using unsafe mutable pointers essentially rips off the guardrails of swift's protection. If you make an indexing mistake, you could end up picking something from system memory that's not really in your array. Swift normally would warn about this, but with unsafe pointers it won't. Advantage is this adds a bit more speed.

Swift's point-wise operators such as .< compares tensors and returns tensors

now TF has TF eager which mostly serves as syntax sugar for the graph. In 2-3 months XLA is coming. In 1 year MLIR will take over XLA and make things much much better.

MLIR will allow you to accelerate stuff to a degree that today is only possible if you write low-level CUDA code. And there's only a few people in the world today who can do that.

current XLA is much more intuitive than the PyTorch JIT.

S4TF development is all done in the open via github, mailing lists, etc.

Jeremy says best way to contribute is pick something you've been doing in Python and create a crappy, slimmed-down Swift version, go from there, and eventually hopefully merge to S4TF. Ask for help on harebrain forum on fast.ai forums while you go.

Fastai S4TF lesson notebooks will be updated continuously as best practices are revised. Keep checking back there to see how Jeremy+Chris recommend S4TF be used to implement neural nets.

S4TF ultimate goal: get native raw TF performance with code written in idiomatic Swift.

Vectorization: take 4 or 8 numbers at a time and throw at a CPU. If GPU you parallelize the vectorization so that multiple 4/8 number sets get calculated at once.

HW Lesson 13:
-Get S4TF env set up


Lesson 14
Deep Learning from the Foundations: fastai + Swift for TensorFlow, part 2
Tuesday, April 30, 2019
-----------------------

Two main approaches to automatically getting computation graphs from eager:
    1. Staging
    2. Tracing

XLA/MLIR stuff is on the horizon, but we can use C/C++/Swift to get pretty high performance right just now!

All C libraries can be imported into Swift using the same formatting. So you can just have a template and plug in any info unique to the C library, and then import it.

Importing C libraries in Python is tough cause the C libraries generate pointers and a bunch of stuff that's hard for Python to manage. Swift doesn't have this problem.

C is important cause lots of libraries written in it. But C is pervasively unsafe, and has weird features and syntax

C++ is simultaneously more important and more gross than C

Inline functions: functions that don't exist anywhere in a program unless you use them

Thanks to Clang, when you import a C library with Swift, it can convert all the C code to Swift code exactly.

Swift can't handle C varargs yet (last C edge case Swift doesn't handle), but there's a workaround: use a header file

To use openCV (which just has a C++ api now) with Swift: use a C header file to disguise the C++ code

Jeremy says you have to know "almost no" C if you want to be able to import into Swift

Why we care about going deep into bottom of numerical computing stack as data scientists: it allows us to do amazing things that we know deliver great results (like sparse convolutions) but are difficult for most folks to implement (cause sparse convolutions require a custom CUDA kernel).

Problem with fastai 1.0's Python datablock API: you have to add each bit in the right order, otherwise you get inscrutable errors.

With Swift, we can implement a DatasetConfig protocol which allows define all of the necessary functions in any order, and reminds you if you've forgotten any one of them.

Protocols: good for modeling different properties of types; help us to safely define behaviors
For e.g.: when you make your type conform to an existing protocol, you get a lot of functionality for free, cause it's all already been defined inside the protocol.

Protocols define behaviors for groups of types. In other words: categorize and describe groups of types. In other words: protocols carry meaning.

Swift extensions are monkey patching on steroids -- except totally safe and encouraged. You can take somebody else's type and make it work with a particular protocol after the fact.

Swift: able to take great ideas from various languages (like Haskell) and make them accessible in a non-weird way (as Haskell also does)

Jeremy says that for him, thinking in terms of protocol-oriented language like Swift, with its bottom-up ways of defining things through protocols and extensions was backwards for him at first -- different from the way he'd been trained to think about things (object-oriented)

Datablocks API in functional-ish, protocol-ish Swift ends up being a lot less code to write than its Python counterpart

In Swift, you do use "â€¢" (option-8) to execute a matrix multiplication (similar to how we use "@" in PyTorch).

struct (value) vs. class (reference)

Beauty of Swift is u can customize how autodiff is done, cause sometimes doing the actual gradient takes too long; but you could put in an approximation which would be good enough. S4TF will let you do that.

In one year or two Swift will give us an infinitely hackable, fully differentiable language. You won't be limited to only trying out the stuff that PyTorch already supports (where your only option otherwise would be to ).

Best way to get involved in S4TF: Jeremy says implement some of your own PyTorch notebooks in S4TF; Chris says write a blogpost cause the S4TF team reads those and it's a great feedback loop for them to make positive changes to the language.

HW Lesson 14:
-Reimplement all S4TF lesson notebooks
-Implement my final solution to probspace KMNIST competition in Swift
-Don't stop blogging/implementing papers/researching-exploring/applying DL to problems/contribute to S4TF

